{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66638,"databundleVersionId":7378729,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Log</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:green\">Download the trainer pack</font>","metadata":{}},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project2/refs/heads/main/trainer.zip\" -O ./trainer.zip\n\n!ls /kaggle/working/\n\n!unzip /kaggle/working/trainer.zip\n\n!rm /kaggle/working/trainer.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">Imports</font>","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\n\nimport os\nimport json\nimport yaml\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport IPython\nfrom PIL import Image\n\nfrom operator import itemgetter\nimport multiprocessing as mp\nmp.set_start_method('spawn', force=True)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import functional as F\n\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import MultiStepLR\n\ntorch.multiprocessing.set_start_method('spawn', force=True)\n\nimport torchvision.models as models\n\n\nfrom trainer import Trainer, hooks, configuration\nfrom trainer.utils import setup_system, patch_configs\nfrom trainer.metrics import AccuracyEstimator\nfrom trainer.tensorboard_visualizer import TensorBoardVisualizer, ModelVisualizer, DataEmbedingVisualizer\nfrom trainer.tensorboard_visualizer import set_writer\nfrom trainer.custom_dataloader import JsonClassificationDataset\nfrom trainer.configuration import load_config_from_yaml\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">Start TensorBoard by Ngrog tunnel</font>\n<a href=\"https://ngrok.com/\">Ngrog tunnel</a>","metadata":{}},{"cell_type":"code","source":"!tensorboard --version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n!tar xf ./ngrok-v3-stable-linux-amd64.tgz -C /usr/local/bin","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Add to the console:**\n\n```cmd\n!ngrok authtoken <authtoken> # /root/.config/ngrok/ngrok.yml\n\n```","metadata":{}},{"cell_type":"code","source":"pool = mp.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir /kaggle/working/log_resnet18 --load_fast=false --host 0.0.0.0 --port 6006 &\",\n                        \"/usr/local/bin/ngrok http 6006 &\"\n                        ]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n    \n```\n\n```\ndef get_data(args1, *agrs):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Class for create JSON file of datasets</font>\n\nWe create Json structure of `train`, `valid` and `test` datasets, for creating PyTorch Dataloader a standard module of `trainer`.","metadata":{}},{"cell_type":"code","source":"class Transform_dataset:\n    def __init__(self, root_dir, save_dir, name_json_file = 'datasets.json', ratio_split_value = 80):\n        self.root_dir = root_dir\n        self.save_dir = save_dir\n        self.name_json_file = name_json_file\n        self.ratio_split_value = ratio_split_value\n        self.dir_image = os.path.join(self.root_dir, 'images', 'images')\n        self.train_file = os.path.join(self.root_dir, 'train.csv')\n        self.test_file = os.path.join(self.root_dir, 'test.csv')\n        self.sample_file = os.path.join(self.root_dir, 'sample_submission.csv')\n        self.json_file = os.path.join(self.save_dir, self.name_json_file)\n        self.temp_train_file = os.path.join(self.save_dir,'temp_train.csv')\n        self.temp_valid_file = os.path.join(self.save_dir, 'temp_valid.csv')\n        self.temp_test_file = os.path.join(self.save_dir, 'temp_test.csv')\n        self.dict_name_class = {}\n        self.unique_classes_train = None\n        \n    def __get_file_paths(self, directory):\n        file_paths = []\n        \n        for root, _, files in os.walk(directory):\n            for file in files:\n                file_path = os.path.join(root, file)\n                file_paths.append(file_path)\n        return file_paths\n\n    def __selest_class_id(self):\n        train_values = pd.read_csv(self.train_file)\n\n        class_counts_train = train_values['class'].value_counts()\n        self.unique_classes_train = class_counts_train.index.tolist()\n        self.unique_classes_train.sort()\n       \n        \n        for name in self.unique_classes_train:\n            self.dict_name_class[name] = int(self.unique_classes_train.index(name))\n\n    def __train_test_split(self):\n        # Load data from CSV file\n        data = pd.read_csv(self.train_file)\n    \n        # set value for split data\n        value_split = round(self.ratio_split_value/100, 2)\n        \n        # Get number of rows\n        num_rows = len(data)\n        \n        # Calculate number of index of test/train data\n        train_size = int(value_split * num_rows)  # 80% for training\n        test_size = num_rows - train_size\n        \n    \n        # Random index reordering\n        indices = np.random.permutation(num_rows)\n        train_indices = indices[:train_size]\n        test_indices = indices[train_size:]\n        \n        # Create training and test set using indexes\n        train_data = data.iloc[train_indices]\n        test_data = data.iloc[test_indices]\n        \n        # Save split data to new CSV files\n        train_data.to_csv(self.temp_train_file, index=False)\n        test_data.to_csv(self.temp_valid_file, index=False) \n\n    def __get_image_shape(self, image_path):\n        img = Image.open(image_path)\n        w, h = img.size\n        c =  img.mode\n        cc = ''\n        if c == 'RGB':\n            cc = 'RGB'\n        elif c == 'RGBA':\n            cc = 'RGBA'\n        elif c == 'L':\n            cc = 'Grayscale'\n        elif c == '1':\n            cc = 'Grayscale'\n        else:\n            cc = c\n        return (w, h, c)\n\n    def __correct_test_file(self):\n        table_1 = pd.read_csv(self.test_file)\n        table_2 = pd.read_csv(self.sample_file)\n        table_test_data = pd.merge(table_1, table_2, on='id', how='left')\n        table_test_data.to_csv(self.temp_test_file, index=False)\n\n    def __remove_temp_files(self):\n        os.remove(self.temp_train_file)\n        os.remove(self.temp_valid_file)\n        os.remove(self.temp_test_file)\n\n    def process_json(self):\n        self.__train_test_split()\n        self.__correct_test_file()\n        self.__selest_class_id()\n\n        table_train = pd.read_csv(self.temp_train_file) # load csv file of train data\n        table_valid = pd.read_csv(self.temp_valid_file) # load csv file of valid data\n        table_test = pd.read_csv(self.temp_test_file) # load csv file of test data\n\n        # add type data selection\n        table_train['type_dataset'] = 'train'\n        table_valid['type_dataset'] = 'valid'\n        table_test['type_dataset'] = 'test'\n\n        # add all data together\n        df_concat_1 = pd.concat([table_train, table_valid], axis=0)\n        df_concat_2 = pd.concat([df_concat_1, table_test], axis=0)\n\n        df_concat_2['class_idx'] = df_concat_2['class'].map(self.dict_name_class)\n\n        list_path_image = self.__get_file_paths(self.dir_image) # load path of images \n\n        # create table of path images\n        list_id = []\n        list_paths = []\n        list_types = []\n        list_width = []\n        list_height = []\n        list_channel = []\n        list_idx_class = []\n        for img in list_path_image:\n            img_name = os.path.split(img)[1]\n            img_id = int(img_name.split('.')[0])\n            img_suffix = img_name.split('.')[1]\n            img_data = self.__get_image_shape(img)\n\n            list_id.append(img_id)\n            list_paths.append(img)\n            list_types.append(img_suffix)\n            list_width.append(img_data[0])\n            list_height.append(img_data[1])\n            list_channel.append(img_data[2])\n\n        # create image table data\n        image_table = pd.DataFrame({\n            'id': list_id, \n            'path_file': list_paths, \n            'file_type': list_types, \n            'image_width': list_width, \n            'image_height': list_height,\n            'image_channel': list_channel\n        })\n\n        table_all_data = pd.merge(df_concat_2, image_table, on='id', how='left')\n        num_err_val = table_all_data['path_file'].isna().sum()\n\n        dict_data = {'datasets':[{'train':[], 'valid': [], 'test': [], 'class_number': len(self.unique_classes_train), 'names_class': self.unique_classes_train},]}\n\n        for i in range(len(table_all_data)):\n            sub_dict = {}\n            main_dict = {}\n            id_file = int(table_all_data.iloc[i]['id'])\n            class_file = table_all_data.iloc[i]['class']\n            class_idx = int(table_all_data.iloc[i]['class_idx'])\n            type_dataset = table_all_data.iloc[i]['type_dataset']\n            path_file = table_all_data.iloc[i]['path_file']\n            file_type = table_all_data.iloc[i]['file_type']\n            image_width = int(table_all_data.iloc[i]['image_width'])\n            image_height = int(table_all_data.iloc[i]['image_height'])\n            image_channel = table_all_data.iloc[i]['image_channel']\n            name_file = str(id_file) + '.' + file_type\n\n            sub_dict['name'] = name_file\n            sub_dict['class'] = class_file\n            sub_dict['clidx'] = class_idx\n            sub_dict['path'] = path_file\n            sub_dict['type'] = file_type\n            sub_dict['width'] = image_width\n            sub_dict['height'] = image_height\n            sub_dict['channel'] = image_channel\n\n            main_dict[str(id_file)] = sub_dict\n\n            if type_dataset == 'train':\n                dict_data['datasets'][0]['train'].append(main_dict)\n            elif type_dataset == 'valid':\n                dict_data['datasets'][0]['valid'].append(main_dict)\n            elif type_dataset == 'test':\n                dict_data['datasets'][0]['test'].append(main_dict)\n            else:\n                print(table_all_data.iloc[i], 'not add to any dataset!')\n\n        with open(self.json_file, 'w') as f:\n            json.dump(dict_data, f, indent=4)\n\n        print(f'Data were saved to file {self.json_file}')\n\n        self.__remove_temp_files()\n        \n\n    def return_json_address(self):\n        return self.json_file","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nroot_dir = os.path.join('/kaggle','input','opencv-pytorch-classification-project-2')\nsave_dir = os.path.join('/kaggle','working')\nname_json_file = 'datasets.json'\nratio_split_value = 80\n\njson_file = Transform_dataset(\n    root_dir=root_dir, \n    save_dir=save_dir, \n    name_json_file=name_json_file, \n    ratio_split_value=ratio_split_value\n)\njson_file.process_json()\npath_json = json_file.return_json_address()\nprint('Path to the JSON data structure: ', path_json)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Class for create custom dataloader from JSON file of datasets</font>\n\nThis is standart class of our `trainer package`. Class `JsonClassificationDataset` is inside `custom_dataloader.py` module. see: [Github - trainer package](https://github.com/RadimKozl/OpenCV_PyTorch_Project2/tree/main/trainer)","metadata":{}},{"cell_type":"code","source":"json_file = os.path.join('/kaggle','working','datasets.json') # /kaggle/working/datasets.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Function for preview datasets</font>","metadata":{}},{"cell_type":"code","source":"def plot_images(loader):\n    # Plot few images\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    plt.figure\n    for images, labels in loader:\n        for i in range(len(labels)):\n            plt.subplot(3, 5, i+1)\n            img = F.to_pil_image(images[i])\n            plt.imshow(img)\n            plt.gca().set_title('Target: {0}'.format(labels[i]))\n            plt.axis('off')\n        plt.show()\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_preprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor()\n    ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Create and show train dataset</font>","metadata":{}},{"cell_type":"code","source":"train_dataset =  JsonClassificationDataset(json_file, type_data='train', dataset_number=0, transform=base_preprocess)\n\n# dataloader with dataset\ntrain_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=15,\n        shuffle=True,\n        num_workers=2\n    )\n\n\n# view dataset\nplot_images(train_loader)\n\n# mean and std values of dataset\ntrain_norm_parameter = train_dataset.calculate_mean_std_manual()\n\n# max number of samples of dataset\ntrain_len_param_dataset = len(train_dataset)\nprint('Number of samples of train dataset: ', train_len_param_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Create and show valid dataset</font>","metadata":{}},{"cell_type":"code","source":"valid_dataset =  JsonClassificationDataset(json_file, type_data='valid', dataset_number=0, transform=base_preprocess)\n\n# dataloader with dataset\nvalid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=15,\n        shuffle=True,\n        num_workers=2\n    )\n\n# view dataset\nplot_images(valid_loader)\n\n# mean and std values of dataset\nvalid_norm_parameter = valid_dataset.calculate_mean_std_manual()\n\n# max number of samples of dataset\nvalid_len_param_dataset = len(valid_dataset)\nprint('Number of samples of valid dataset: ', valid_len_param_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Create and show test dataset</font>","metadata":{}},{"cell_type":"code","source":"test_dataset =  JsonClassificationDataset(json_file, type_data='test', dataset_number=0, transform=base_preprocess)\n\n# dataloader with dataset\ntest_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=15,\n        shuffle=True,\n        num_workers=2\n    )\n\n# view dataset\nplot_images(test_loader)\n\n# mean and std values of dataset\ntest_norm_parameter = test_dataset.calculate_mean_std_manual()\n\n# max number of samples of dataset\ntest_len_param_dataset = len(test_dataset)\nprint('Number of samples of test dataset: ', test_len_param_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Create function for get datasets for training</font>","metadata":{}},{"cell_type":"code","source":"def get_train_data(batch_size, writer, norm_parameter=None, json_file='datasets.json', num_workers=1, data_augmentation=False):\n    \n    if norm_parameter is not None:\n        mean, std = norm_parameter\n        # common transforms\n        common_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n    else:\n        # common transforms\n        common_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor()\n        ])\n        \n    # if data_augmentation is true \n    # data augmentation implementation\n    if data_augmentation:\n        train_transforms = transforms.Compose([\n            transforms.RandomChoice([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomVerticalFlip(),\n                transforms.RandomRotation(20, fill=(0,)),\n                transforms.RandomPerspective(distortion_scale=0.6, p=1),\n                transforms.ColorJitter(brightness=.5, hue=.3)\n            ]),\n            common_transforms\n        ])\n    # else do common transforms\n    else:\n        train_transforms = common_transforms\n\n    # train dataloader\n    train_dataset =  JsonClassificationDataset(json_file, type_data='train', dataset_number=0, transform=train_transforms)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers\n    )\n    \n\n    # test dataloader\n    valid_dataset =  JsonClassificationDataset(json_file, type_data='valid', dataset_number=0, transform=common_transforms)\n    \n    test_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers\n    )\n\n   \n \n    add_data_embedings = DataEmbedingVisualizer(valid_dataset, writer)\n    add_data_embedings.update_charts()\n    \n    return train_loader, test_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Create function for get dataset for testing model</font>","metadata":{}},{"cell_type":"code","source":"def get_test_data(batch_size, norm_parameter=None, json_file='datasets.json', num_workers=1):\n    \n    if norm_parameter is not None:\n        mean, std = norm_parameter\n        # common transforms\n        common_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n    else:\n        # common transforms\n        common_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor()\n        ])\n        \n    \n    # test dataloader\n    test_dataset =  JsonClassificationDataset(json_file, type_data='test', dataset_number=0, transform=common_transforms)\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers\n    )\n    return test_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/opencv-pytorch-classification-project-2/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Update configuration module of trainer</font>\n\nWe update setting of configuration classes by YAML file, see: [GitHub - configuration.py](https://github.com/RadimKozl/OpenCV_PyTorch_Project2/blob/main/trainer/configuration.py)\n\n### <font style=\"color:blue\">Download config file</font>","metadata":{}},{"cell_type":"code","source":"!wget \"https://raw.githubusercontent.com/RadimKozl/OpenCV_PyTorch_Project2/refs/heads/main/config.yaml\" -O /kaggle/working/config.yaml\n!mv /kaggle/working/config.yaml /kaggle/working/config_experiment_resnet18.yaml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Update config file</font>","metadata":{}},{"cell_type":"code","source":"config_ex_path_resnet18 = os.path.join('/kaggle','working','config_experiment_resnet18.yaml') # path of yaml file\n\n# load yaml file\nwith open(config_ex_path_resnet18, 'r') as f:\n    config = yaml.safe_load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set system config values\nconfig['system']['seed'] = 21\nconfig['system']['cudnn_benchmark_enabled'] = False\nconfig['system']['cudnn_deterministic'] = False\n\n# set dataset config values\nconfig['dataset']['root_dir'] = \"/kaggle/working/\"\nconfig['dataset']['json_file'] = \"datasets.json\"\n\n# set dataloader config values\nconfig['dataloader']['batch_size'] = 50\nconfig['dataloader']['num_workers'] = 2\n\n# set optimizer config values\nconfig['optimizer']['learning_rate'] = 0.001\nconfig['optimizer']['momentum'] = 0.9\nconfig['optimizer']['weight_decay'] = 0.0001\nconfig['optimizer']['lr_step_milestones'] = [30, 40]\nconfig['optimizer']['lr_gamma'] = 0.1\n\n# set trainer config values\nconfig['trainer']['model_dir'] = \"checkpoints\"\nconfig['trainer']['model_saving_frequency'] = 1\nconfig['trainer']['device'] = \"cuda\"\nconfig['trainer']['epoch_num'] = 50\nconfig['trainer']['log_interval'] = 5\nconfig['trainer']['test_interval'] = 1\nconfig['trainer']['progress_bar'] = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(config_ex_path_resnet18, 'w') as f:\n    yaml.dump(config, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <font style=\"color:blue\">Load Config data from file</font>","metadata":{}},{"cell_type":"code","source":"system_config, dataset_config, dataloader_config, optimizer_config, trainer_config = load_config_from_yaml(config_ex_path_resnet18)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{}},{"cell_type":"code","source":"def pretrained_resnet18(transfer_learning=True, fine_tune_from_layer3=False, num_class=13):\n    \"\"\"\n    Load a pretrained ResNet18 model and configure it for transfer learning or fine-tuning.\n    \n    Args:\n        transfer_learning (bool): Whether to freeze the entire model except the final layer.\n        fine_tune_from_layer3 (bool): If True, fine-tune layers after Layer 3 (Layer 4 + FC layer).\n        num_class (int): Number of output classes for the final layer.\n        \n    Returns:\n        resnet: A modified ResNet18 model ready for transfer learning or fine-tuning.\n    \"\"\"\n    # Loading the pretrained model\n    resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    \n    # Freeze the entire model if transfer_learning is True\n    if transfer_learning:\n        for param in resnet.parameters():\n            param.requires_grad = False\n    \n    # If fine_tune_from_layer3 is True, we unlock layers after Layer 3 (ie Layer 4 and beyond)\n    if fine_tune_from_layer3:\n        for param in resnet.layer4.parameters():  # We unblock the Layer 4 weights\n            param.requires_grad = True\n    \n    # Replacing the last fully connected layer by the number of classes\n    last_layer_in = resnet.fc.in_features\n    resnet.fc = nn.Linear(last_layer_in, num_class)\n    \n    # Unlocking the last layer (classifier)\n    for param in resnet.fc.parameters():\n        param.requires_grad = True\n\n    return resnet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model set for transfer learning\nmodel_resnet18_TL = pretrained_resnet18(transfer_learning=True, fine_tune_from_layer3=False, num_class=13)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model set for fine tuning\nmodel_resnet18_FT = pretrained_resnet18(transfer_learning=True, fine_tune_from_layer3=False, num_class=13)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{}},{"cell_type":"code","source":"class ExperimentTL:\n    def __init__(\n        self,\n        system_config: configuration.SystemConfig = configuration.SystemConfig(),\n        dataset_config: configuration.DatasetConfig = configuration.DatasetConfig(),\n        dataloader_config: configuration.DataloaderConfig = configuration.DataloaderConfig(),\n        optimizer_config: configuration.OptimizerConfig = configuration.OptimizerConfig()\n    ):\n        self.tb_writer = set_writer('log_resnet18/transfer_learning')\n        self.loader_train, self.loader_test = get_train_data(\n            batch_size=dataloader_config.batch_size,\n            norm_parameter = train_norm_parameter,\n            data_augmentation = True,\n            num_workers=dataloader_config.num_workers,\n            json_file='/kaggle/working/datasets.json',\n            writer=self.tb_writer\n        )\n        \n        setup_system(system_config)\n\n        self.model = model_resnet18_FT\n            \n        self.loss_fn = nn.CrossEntropyLoss()\n        self.metric_fn = AccuracyEstimator(topk=(1, ))\n        self.optimizer = optim.SGD(\n            self.model.parameters(),\n            lr=optimizer_config.learning_rate,\n            weight_decay=optimizer_config.weight_decay,\n            momentum=optimizer_config.momentum\n        )\n        self.lr_scheduler = MultiStepLR(\n            self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n        )\n        \n        self.visualizer = TensorBoardVisualizer(writer=self.tb_writer)\n\n    def run(self, trainer_config: configuration.TrainerConfig) -> dict:\n\n        device = torch.device(trainer_config.device)\n        self.model = self.model.to(device)\n        \n        # add network graph with inputs info\n        images, labels = next(iter(self.loader_test))\n        images = images.to(trainer_config.device)\n        add_network_graph_tensorboard = ModelVisualizer(self.model, images, self.tb_writer)\n        add_network_graph_tensorboard.update_charts()\n        \n        self.loss_fn = self.loss_fn.to(device)\n\n        model_trainer = Trainer(\n            model=self.model,\n            loader_train=self.loader_train,\n            loader_test=self.loader_test,\n            loss_fn=self.loss_fn,\n            metric_fn=self.metric_fn,\n            optimizer=self.optimizer,\n            lr_scheduler=self.lr_scheduler,\n            device=device,\n            data_getter=itemgetter(0),\n            target_getter=itemgetter(1),\n            stage_progress=trainer_config.progress_bar,\n            get_key_metric=itemgetter(\"top1\"),\n            visualizer=self.visualizer,\n            model_saving_frequency=trainer_config.model_saving_frequency,\n            save_dir=trainer_config.model_dir\n        )\n\n        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n        return self.metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    '''Run the experiment\n    '''\n    # patch configs depending on cuda availability\n    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=15)\n    dataset_config = configuration.DatasetConfig(root_dir=\"/kaggle/working/\")\n    experiment = ExperimentTL(dataset_config=dataset_config, dataloader_config=dataloader_config)\n    results = experiment.run(trainer_config)\n\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Log [5 Points]</font>\n\n**Share your TensorBoard scalars logs here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\n<font style=\"color:red\">Note:</font> In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"pool = mp.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir ./logs_fashion_mnist --load_fast=false --host 0.0.0.0 --port 6006 &\",\n                        \"/usr/local/bin/ngrok http 6006 &\"\n                        ]]## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{}}]}