{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66638,"databundleVersionId":7378729,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Log</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n    \n```\n\n```\ndef get_data(args1, *agrs):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{}},{"cell_type":"markdown","source":"### ***Class for create JSON file of datasets***\n\nWe create Json structure of `train`, `valid` and `test` datasets, for creating PyTorch Dataloader a standard module of `trainer`.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib as plt\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T14:50:43.490009Z","iopub.execute_input":"2024-10-11T14:50:43.490938Z","iopub.status.idle":"2024-10-11T14:50:44.591694Z","shell.execute_reply.started":"2024-10-11T14:50:43.490892Z","shell.execute_reply":"2024-10-11T14:50:44.590566Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class Transform_dataset:\n    def __init__(self, root_dir, save_dir, name_json_file = 'datasets.json', ratio_split_value = 80):\n        self.root_dir = root_dir\n        self.save_dir = save_dir\n        self.name_json_file = name_json_file\n        self.ratio_split_value = ratio_split_value\n        self.dir_image = os.path.join(self.root_dir, 'images', 'images')\n        self.train_file = os.path.join(self.root_dir, 'train.csv')\n        self.test_file = os.path.join(self.root_dir, 'test.csv')\n        self.sample_file = os.path.join(self.root_dir, 'sample_submission.csv')\n        self.json_file = os.path.join(self.save_dir, self.name_json_file)\n        self.temp_train_file = os.path.join(self.save_dir,'temp_train.csv')\n        self.temp_valid_file = os.path.join(self.save_dir, 'temp_valid.csv')\n        self.temp_test_file = os.path.join(self.save_dir, 'temp_test.csv')\n        self.dict_name_class = {}\n        self.unique_classes_train = None\n        \n    def __get_file_paths(self, directory):\n        file_paths = []\n        \n        for root, _, files in os.walk(directory):\n            for file in files:\n                file_path = os.path.join(root, file)\n                file_paths.append(file_path)\n        return file_paths\n\n    def __selest_class_id(self):\n        train_values = pd.read_csv(self.train_file)\n\n        class_counts_train = train_values['class'].value_counts()\n        self.unique_classes_train = class_counts_train.index.tolist()\n        self.unique_classes_train.sort()\n       \n        \n        for name in self.unique_classes_train:\n            self.dict_name_class[name] = int(self.unique_classes_train.index(name))\n\n    def __train_test_split(self):\n        # Load data from CSV file\n        data = pd.read_csv(self.train_file)\n    \n        # set value for split data\n        value_split = round(self.ratio_split_value/100, 2)\n        \n        # Get number of rows\n        num_rows = len(data)\n        \n        # Calculate number of index of test/train data\n        train_size = int(value_split * num_rows)  # 80% for training\n        test_size = num_rows - train_size\n        \n    \n        # Random index reordering\n        indices = np.random.permutation(num_rows)\n        train_indices = indices[:train_size]\n        test_indices = indices[train_size:]\n        \n        # Create training and test set using indexes\n        train_data = data.iloc[train_indices]\n        test_data = data.iloc[test_indices]\n        \n        # Save split data to new CSV files\n        train_data.to_csv(self.temp_train_file, index=False)\n        test_data.to_csv(self.temp_valid_file, index=False) \n\n    def __get_image_shape(self, image_path):\n        img = Image.open(image_path)\n        w, h = img.size\n        c =  img.mode\n        cc = ''\n        if c == 'RGB':\n            cc = 'RGB'\n        elif c == 'RGBA':\n            cc = 'RGBA'\n        elif c == 'L':\n            cc = 'Grayscale'\n        elif c == '1':\n            cc = 'Grayscale'\n        else:\n            cc = c\n        return (w, h, c)\n\n    def __correct_test_file(self):\n        table_1 = pd.read_csv(self.test_file)\n        table_2 = pd.read_csv(self.sample_file)\n        table_test_data = pd.merge(table_1, table_2, on='id', how='left')\n        table_test_data.to_csv(self.temp_test_file, index=False)\n\n    def __remove_temp_files(self):\n        os.remove(self.temp_train_file)\n        os.remove(self.temp_valid_file)\n        os.remove(self.temp_test_file)\n\n    def process_json(self):\n        self.__train_test_split()\n        self.__correct_test_file()\n        self.__selest_class_id()\n\n        table_train = pd.read_csv(self.temp_train_file) # load csv file of train data\n        table_valid = pd.read_csv(self.temp_valid_file) # load csv file of valid data\n        table_test = pd.read_csv(self.temp_test_file) # load csv file of test data\n\n        # add type data selection\n        table_train['type_dataset'] = 'train'\n        table_valid['type_dataset'] = 'valid'\n        table_test['type_dataset'] = 'test'\n\n        # add all data together\n        df_concat_1 = pd.concat([table_train, table_valid], axis=0)\n        df_concat_2 = pd.concat([df_concat_1, table_test], axis=0)\n\n        df_concat_2['class_idx'] = df_concat_2['class'].map(self.dict_name_class)\n\n        list_path_image = self.__get_file_paths(self.dir_image) # load path of images \n\n        # create table of path images\n        list_id = []\n        list_paths = []\n        list_types = []\n        list_width = []\n        list_height = []\n        list_channel = []\n        list_idx_class = []\n        for img in list_path_image:\n            img_name = os.path.split(img)[1]\n            img_id = int(img_name.split('.')[0])\n            img_suffix = img_name.split('.')[1]\n            img_data = self.__get_image_shape(img)\n\n            list_id.append(img_id)\n            list_paths.append(img)\n            list_types.append(img_suffix)\n            list_width.append(img_data[0])\n            list_height.append(img_data[1])\n            list_channel.append(img_data[2])\n\n        # create image table data\n        image_table = pd.DataFrame({\n            'id': list_id, \n            'path_file': list_paths, \n            'file_type': list_types, \n            'image_width': list_width, \n            'image_height': list_height,\n            'image_channel': list_channel\n        })\n\n        table_all_data = pd.merge(df_concat_2, image_table, on='id', how='left')\n        num_err_val = table_all_data['path_file'].isna().sum()\n\n        dict_data = {'datasets':[{'train':[], 'valid': [], 'test': [], 'class_number': len(self.unique_classes_train), 'names_class': self.unique_classes_train},]}\n\n        for i in range(len(table_all_data)):\n            sub_dict = {}\n            main_dict = {}\n            id_file = int(table_all_data.iloc[i]['id'])\n            class_file = table_all_data.iloc[i]['class']\n            class_idx = int(table_all_data.iloc[i]['class_idx'])\n            type_dataset = table_all_data.iloc[i]['type_dataset']\n            path_file = table_all_data.iloc[i]['path_file']\n            file_type = table_all_data.iloc[i]['file_type']\n            image_width = int(table_all_data.iloc[i]['image_width'])\n            image_height = int(table_all_data.iloc[i]['image_height'])\n            image_channel = table_all_data.iloc[i]['image_channel']\n            name_file = str(id_file) + '.' + file_type\n\n            sub_dict['name'] = name_file\n            sub_dict['class'] = class_file\n            sub_dict['clidx'] = class_idx\n            sub_dict['path'] = path_file\n            sub_dict['type'] = file_type\n            sub_dict['width'] = image_width\n            sub_dict['height'] = image_height\n            sub_dict['channel'] = image_channel\n\n            main_dict[str(id_file)] = sub_dict\n\n            if type_dataset == 'train':\n                dict_data['datasets'][0]['train'].append(main_dict)\n            elif type_dataset == 'valid':\n                dict_data['datasets'][0]['valid'].append(main_dict)\n            elif type_dataset == 'test':\n                dict_data['datasets'][0]['test'].append(main_dict)\n            else:\n                print(table_all_data.iloc[i], 'not add to any dataset!')\n\n        with open(self.json_file, 'w') as f:\n            json.dump(dict_data, f, indent=4)\n\n        print(f'Data were saved to file {self.json_file}')\n\n        self.__remove_temp_files()\n        \n\n    def return_json_address(self):\n        return self.json_file","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:50:44.593951Z","iopub.execute_input":"2024-10-11T14:50:44.594473Z","iopub.status.idle":"2024-10-11T14:50:44.634103Z","shell.execute_reply.started":"2024-10-11T14:50:44.594433Z","shell.execute_reply":"2024-10-11T14:50:44.632894Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%time\nroot_dir = os.path.join('/kaggle','input','opencv-pytorch-classification-project-2')\nsave_dir = os.path.join('/kaggle','working')\nname_json_file = 'datasets.json'\nratio_split_value = 80\n\njson_file = Transform_dataset(root_dir=root_dir, save_dir=save_dir, name_json_file=name_json_file, ratio_split_value=ratio_split_value)\njson_file.process_json()\npath_json = json_file.return_json_address()\nprint('Path to the JSON data structure: ', path_json)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T14:50:44.635457Z","iopub.execute_input":"2024-10-11T14:50:44.636274Z","iopub.status.idle":"2024-10-11T14:51:44.038070Z","shell.execute_reply.started":"2024-10-11T14:50:44.636218Z","shell.execute_reply":"2024-10-11T14:51:44.036880Z"}},"outputs":[{"name":"stdout","text":"Data were saved to file /kaggle/working/datasets.json\nPath to the JSON data structure:  /kaggle/working/datasets.json\nCPU times: user 8.52 s, sys: 2.15 s, total: 10.7 s\nWall time: 59.4 s\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/opencv-pytorch-classification-project-2/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Log [5 Points]</font>\n\n**Share your TensorBoard scalars logs here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\n<font style=\"color:red\">Note:</font> In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{}}]}